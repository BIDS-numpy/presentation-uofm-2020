{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NumPy: A look at the past, present, and future of array computation\n",
    "\n",
    "Ross Barnowski `rossbar@berkeley.edu` | [rossbar](https://github.com/rossbar) on GitHub\n",
    "\n",
    "University of Michigan EECS | 1/30/2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What is NumPy?\n",
    "\n",
    "> *NumPy is the fundamental package for scientific computing with Python*\n",
    "> \n",
    ">  [numpy.org](https://numpy.org/)\n",
    "\n",
    "Strong stuff."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The scientific Python ecosystem\n",
    "\n",
    "![scientific_python_ecosystem](images/state_of_the_stack_2015.png)\n",
    "\n",
    "Image credit: [Jake VanderPlas](http://vanderplas.com/) circa 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Code example: github graphql query for top starred projects with numpy as a dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# At a glance\n",
    "\n",
    "![numpy_overview](images/numpy_overview_graphic.png)\n",
    "\n",
    "Image credit: [Shaloo Shalini](https://www.linkedin.com/in/shalooshalini/): [@shaloo](https://github.com/shaloo) on GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A bit of history\n",
    "\n",
    " - **Mid 90's/Early 00's**: desire for high-performance numerical computation in python eventually culminates in the [`Numeric`](https://numpy.org/_downloads/768fa66c250a0335ad3a6a30fae48e34/numeric-manual.pdf) library\n",
    " - Early adopters included the [Space Telescope Science Institute (STScI)](http://www.stsci.edu/) who developed another array computation package to better suit their needs: `Numarray`.\n",
    " - **2005** The best ideas from `Numeric` and `Numarray` were combined in the development of a new library, `numpy`\n",
    "   * This work was largely done by [Travis Oliphant](https://github.com/teoliphant), then an assistant professor at BYU\n",
    " - **2006** Numpy v1.0 released in October\n",
    " \n",
    "[NumPy Development History](https://github.com/numpy/numpy/graphs/contributors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What does NumPy provide?\n",
    "\n",
    " - `ndarray`: A generic, n-dimensional array data structure\n",
    " - Sophisticated machinery for operating on array data (vectorization, broadcasting, `ufuncs`)\n",
    " - Language extension/integration (C-API, `f2py`)\n",
    "   * [Array API](https://docs.scipy.org/doc/numpy/reference/c-api.array.html) for accessing/extending array functionality in external libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What else?\n",
    "\n",
    "For historical reasons, `numpy` also includes tools for common scientific/numerical tasks:\n",
    "   * Random number generation (`np.random`)\n",
    "   * Fourier analysis (`np.fft`)\n",
    "   * Linear algebra (`np.linalg`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The `scipy` package includes modules with the same name? What's the deal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import scipy, scipy.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(scipy.random)\n",
    "scipy.random is np.random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(scipy.fft)\n",
    "scipy.fft is np.fft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(scipy.linalg)\n",
    "scipy.linalg is np.linalg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<font color=\"red\">Check with team</font>\n",
    "\n",
    "NumPy will continue to *support* the `fft` and `linalg` modules, but the scipy versions should probably be preferred for new code:\n",
    " - Optimizations/enhancements\n",
    " - More capabilities: e.g. see [this quick comparison](https://numpy.org/devdocs/reference/routines.linalg.html) of the `numpy` and `scipy` `linalg` modules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Where is NumPy used?\n",
    "\n",
    "### <font color=red> Investigate the following for appropriately sized examples </font>\n",
    "\n",
    " - To produce the first image of a black hole \n",
    "   [Event Horizon Telescope Collaboration](https://github.com/achael/eht-imaging)\n",
    " - [To detect the gravitational wave signature from a neutron star merger](https://github.com/gwastro/pycbc)\n",
    " - [To discover fundamental particles like the Higgs Boson](https://github.com/cms-sw/cmssw)\n",
    "   * Also [scikit-hep](https://scikit-hep.org/)\n",
    " - [Neuroimaging](https://nipy.org/nibabel/) - nipy uses `ndarray` as the fundamental structure for the entire stack\n",
    "   * fMRI visualization example from [section 3.4](https://www.frontiersin.org/articles/10.3389/fninf.2014.00014/full#h4)\n",
    "     is a nice, brief example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neuroimaging Analysis\n",
    "\n",
    "Like much of the scientific python ecosystem, the [nipy](https://nipy.org/) relies on `np.ndarray` as the fundamental structure for neuroimaging data.\n",
    "\n",
    "The following example is adapted from [Machine learning for neuroimaging with scikit learn](https://www.frontiersin.org/articles/10.3389/fninf.2014.00014/full). The dataset used comes from the [nilearn data](https://www.nitrc.org/frs/?group_id=728).\n",
    "\n",
    "<font color=red>**Add example of loading full Nifti image to show 4D structure of data?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import nibabel   # package for loading/saving neuroimaging data\n",
    "bg_img = nibabel.load('data/bg.nii.gz')\n",
    "bg = bg_img.get_fdata()\n",
    "type(bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Create activation map by thresholding the data\n",
    "act_thresh = 6000\n",
    "act = bg.copy()\n",
    "# Set \"unactivated\" voxels to NaN for visualization\n",
    "act[act <= act_thresh] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# imshow kwargs\n",
    "imshow_opts = {\n",
    "    \"origin\" : \"lower\",\n",
    "    \"interpolation\" : \"nearest\"\n",
    "}\n",
    "\n",
    "# Axial slice of activation map overlay\n",
    "plt.imshow(bg[...,10].T, cmap=\"gray\");             # Background\n",
    "plt.imshow(act[...,10].T, cmap=\"plasma\");          # Activation map\n",
    "plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detecting gravitational wave signature of black hole and neutron star mergers\n",
    "\n",
    "[PyCBC](https://pycbc.org/) is the toolkit used to analyze data from gravitational wave observatories like [LIGO](https://www.ligo.caltech.edu/) and [Virgo](http://www.virgo-gw.eu/).\n",
    "\n",
    "The [PyCBC tutorials](https://github.com/gwastro/PyCBC-Tutorials) have some really cool examples - let's recreate the \"chirp\" from [first ever direct detection of gravitational waves](https://en.wikipedia.org/wiki/First_observation_of_gravitational_waves) that resulted from two black holes merging. For more info, see [the second PyCBC tutorial](https://colab.research.google.com/github/gwastro/pycbc-tutorials/blob/master/tutorial/2_VisualizationSignalProcessing.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import pycbc\n",
    "from pycbc import catalog\n",
    "\n",
    "merger_data = catalog.Merger('GW150914')\n",
    "# Though the catalog includes data from multiple observatories,\n",
    "# let's focus on just one\n",
    "ligo_data = merger_data.strain('L1')\n",
    "type(ligo_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "`pycbc` has its own (quite extensive) API that uses `numpy` and `scipy` under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(type(ligo_data._data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pycbc.types.aligned.ArrayWithAligned.__bases__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To re-create the \"chirp\" we have to do some analysis on the raw data. `pycbc` relies on tools in `scipy.fft` and `scipy.signal` to implement the frequency analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Flatten frequency \n",
    "res = ligo_data.whiten(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "time_of_merger = merger_data.time\n",
    "\n",
    "# Look 100 msec-worth of data around the merger time\n",
    "roi = res.time_slice(time_of_merger - 0.1, time_of_merger + 0.1)\n",
    "\n",
    "# Similar to a spectrogram with more sophisticated, irregular sampling\n",
    "times, freqs, power = roi.qtransform(\n",
    "    delta_t=0.001,\n",
    "    logfsteps=100,\n",
    "    qrange=(8, 8),\n",
    "    frange=(30, 512),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6,3))\n",
    "ax.pcolormesh(times, freqs, power**0.5)\n",
    "ax.set_yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parker Solar Probe\n",
    "\n",
    "The [Parker Solar Probe](https://www.nasa.gov/content/goddard/parker-solar-probe-humanity-s-first-visit-to-a-star) was launched in 2018 to study the solar atmosphere, coming nearer to the sun than any previous space probe. Data from the Parker probe is already yielding [unexpected results](https://news.engin.umich.edu/2019/12/were-missing-something-fundamental-about-the-sun/?utm_source=newsletter&utm_medium=email&utm_campaign=January_2020).\n",
    "\n",
    "Full disclosure: the original analysis for the [results published in Nature](https://www.nature.com/articles/s41586-019-1813-z) were produced with IDL, not Python. However, NASA just released [the first batch of data from the probe](https://sppgway.jhuapl.edu/) to the public, so let's see if we can't replicate some results..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "One of the instruments on the probe is [SWEAP](http://sweap.cfa.harvard.edu/), a set of charged-particle detectors. Publicly available data from SWEAP can be found [here](http://sweap.cfa.harvard.edu/Data.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A quick aside: data formats\n",
    "\n",
    "Data from the Parker probe is stored in NASA's [Common Data Format (CDF)](https://cdf.gsfc.nasa.gov/). Python libraries such as [spacepy](https://spacepy.github.io/) are used for I/O from the CDF format. As you might expect, `spacepy`'s `pycdf` module loads data from CDF files into NumPy arrays. Unfortunately, `spacepy`'s `pycdf` module depends on an external C-library, and there is not (yet) a `conda` recipe for installing it automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "To get around this I've used `spacepy.pycdf` to save a small amount of SWEAP in the more Python-friendly `.npz` format. I took data from the [SPC instrument collected on 11-08-18](http://sweap.cfa.harvard.edu/pub/data/sci/sweap/spc/L2/2018/11/). If you'd like to work with the full dataset, you can [manually install CDF](https://spacepy.github.io/install_linux.html#cdf), download the data (or any other dataset), and use `devlogs/parker_probe_velocity_log.py` as an example of how to load and interact with the raw data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# The dataset is ~150MB, so it is not included in the repo.\n",
    "\n",
    "import os, requests, tqdm\n",
    "fname = 'data/parker_probe_spcL2_data_11-08-18.npz'\n",
    "dsize = 152538956   # File size in bytes\n",
    "dlink = 'https://www.dropbox.com/s/z45tbkqwjpyu6tz/parker_probe_spcL2_data_11-08-18.npz?dl=0'\n",
    "if not os.path.exists(fname):\n",
    "    r = requests.get(\n",
    "        dlink,\n",
    "        headers={'user-agent':'Wget/1.20 (linux-gnu)'},\n",
    "        stream=True\n",
    "    )\n",
    "    with open(fname, 'wb') as fh:\n",
    "        for chunk in tqdm.tqdm(r.iter_content(chunk_size=1024), total=dsize/1024):\n",
    "            if chunk:\n",
    "                fh.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we have the data, let's try to replicate the top pane of [this image](http://sweap.cfa.harvard.edu/Images/example_spc_ql.png) from the [SWEAP data page](http://sweap.cfa.harvard.edu/Data.html).\n",
    "\n",
    "We don't have time to discuss the data in detail, but the [Appendix 3 of the SWEAP Data User's Guide](http://sweap.cfa.harvard.edu/sweap_data_user_guide.pdf) outlines a procedure we can use to reproduce the desired figure. We start by loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Load data from the SPC instrument on the Parker probe\n",
    "data = np.load(fname)\n",
    "t = data['t']          # Measurement time\n",
    "# Edges of Voltage bins\n",
    "mv_lo = data['mv_lo'].T\n",
    "mv_hi = data['mv_hi'].T\n",
    "# Differential charge flux density\n",
    "dcfd = data['diff_charge_flux_density'].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# The data include timestamps with microsecond resolution\n",
    "# and 128 channels per data point\n",
    "print(t.shape, mv_lo.shape, dcfd.shape)\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# The CDF file uses a fill value (-1e31) to denote invalid data\n",
    "print(dcfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Let's replace them so we can keep track of non-data in the arrays\n",
    "for arr in (mv_lo, mv_hi, dcfd):\n",
    "    arr[arr == -1e31] = np.nan\n",
    "print(dcfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Upon closer inspection, only the first 30 of the 128 channels store valid data\n",
    "np.sum(np.isfinite(dcfd), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Limit computation to valid voltage bins\n",
    "mv_lo, mv_hi, dcfd = mv_lo[:31,:], mv_hi[:31,:], dcfd[:31,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After removing the unused data channels, there are still individual measurements that resulted in invalid data. Let's remove these as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Mask out time samples that have bad data\n",
    "bad_data = np.any(~np.isfinite(dcfd), axis=0)\n",
    "t = t[~bad_data]\n",
    "mv_lo = mv_lo[:, ~bad_data]\n",
    "mv_hi = mv_hi[:, ~bad_data]\n",
    "dcfd = dcfd[:, ~bad_data]\n",
    "print(\"{} time samples out of {} discarded\".format(bad_data.sum(), bad_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That takes care of the data munging, now on to the computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Compute center and widths of voltage bins\n",
    "V = (mv_hi + mv_lo) / 2\n",
    "dV = (mv_hi - mv_lo) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Proton properties\n",
    "q = 1.602e-19   # Charge [C]\n",
    "mp = 1.673e-27  # Mass [kg]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# N.B. scipy.special includes functions for computing the elliptic integral\n",
    "# For the sake of consistency, we use the approximation as detailed in App. 3\n",
    "from elliptic_integral_approximation import schlawin as elliptic\n",
    "\n",
    "# Convert from energy -> velocity\n",
    "v = np.sqrt(2 * q * mv_hi / mp) * (2 / np.pi) * elliptic(mv_lo/mv_hi)\n",
    "dv = np.sqrt((4 * (q/mp) * V) - v**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Finally, compute the proton distribution as a function of proton velocity\n",
    "Fv = (dcfd / (q* v* 10**8)) * (1 / dv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's see how we did..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "viz_kwargs = {\n",
    "    \"cmap\" : plt.cm.plasma,\n",
    "    \"norm\" : colors.LogNorm(vmin=1, vmax=200)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize\n",
    "fig = plt.figure(figsize=(8,4)); ax = fig.add_subplot(111)\n",
    "ax.pcolormesh(t[np.newaxis,:], v/1000, Fv, **viz_kwargs)\n",
    "fig.colorbar(ax.collections[0])\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Scope of NumPy\n",
    "\n",
    "NumPy currently targets computation involving:\n",
    "\n",
    " * in-memory, homogenously-typed array data\n",
    " * cpu-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Important guiding principles:\n",
    " - **Stability**: Foundational component of the scientific python ecosystem for going-on 15 years\n",
    " - **Interoperability**\n",
    "   * NumPy is the standard array data structure within the scientific Python ecosystem\n",
    "   * What about all the new array libraries?\n",
    "     - [XArray](http://xarray.pydata.org/en/stable/)\n",
    "     - [Dask Arrays](https://docs.dask.org/en/latest/array.html)\n",
    "     - [Jax](https://jax.readthedocs.io/en/latest/)\n",
    "     - [pydata sparse](https://sparse.pydata.org/en/latest/)\n",
    "     - [PyTorch](https://pytorch.org/)\n",
    "     - [TensorFlow](https://www.tensorflow.org/api_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The changing landscape\n",
    "\n",
    " - In the early days, many new NumPy users were converts from languages like Matlab and IDL\n",
    "   * See the [NumPy for Matlab users](https://docs.scipy.org/doc/numpy/user/numpy-for-matlab-users.html) article in the docs\n",
    "   \n",
    " - **Now**: The scientific Python ecosystem (including libraries for data science and ML) is incredibly feature-rich and powerful, and is attracting many new users.\n",
    "   * Users interested in specific applications (machine learning, image processing, geoscience, bioinformatics, etc.) end up interacting with NumPy indirectly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Google Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Data downloaded from google trends\n",
    "!ls data/*.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!head data/datascience.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "timeseries_dtype = np.dtype([\n",
    "    ('date', 'datetime64[M]'),\n",
    "    ('relpop', float)\n",
    "])\n",
    "\n",
    "parse_kwargs = {\n",
    "    \"skiprows\" : 3,\n",
    "    \"delimiter\" : \",\",\n",
    "    \"dtype\" : timeseries_dtype\n",
    "}\n",
    "\n",
    "fnames = (\"numpy\", \"datascience\", \"matlab\")\n",
    "\n",
    "data = {\n",
    "    fname : np.loadtxt(\"data/{}.csv\".format(fname), **parse_kwargs) for fname in fnames\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for name, vals in data.items():\n",
    "    plt.plot(vals['date'], vals['relpop'], label=name)\n",
    "ax.set_title('Google Trends (US): 2004 - Present')\n",
    "ax.set_ylabel('Relative Popularity of Search Term [arb]')\n",
    "fig.autofmt_xdate()\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def smooth(s, kernsize=21):\n",
    "    s_padded = np.r_[s[kernsize-1:0:-1], s, s[-2:-kernsize-1:-1]]\n",
    "    kern = np.hamming(kernsize)\n",
    "    res_padded = np.convolve(kern/kern.sum(), s_padded, mode='valid')\n",
    "    # De-pad and renormalize\n",
    "    return 100 * res_padded[kernsize//2:-kernsize//2+1] / res_padded.max()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for name, vals in data.items():\n",
    "    plt.plot(vals['date'], smooth(vals['relpop']), label=name)\n",
    "ax.set_title('Google Trends (US): 2004 - Present')\n",
    "ax.set_ylabel('Relative Popularity of Search Term [arb]')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The changing landscape \n",
    " * Focus resources on supporting stable, performant base for dependent libraries\n",
    "   * Scope: what goes in NumPy itself vs. dependent packages?\n",
    "   * Balance between performance and maintainability\n",
    "     > Optimization is the altar where maintainability is sacrificed\n",
    "     >\n",
    "     > \\- Luciano Ramalho, *Fluent Python*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How is NumPy Developed\n",
    "\n",
    " - Collaboratively <font color=\"red\">(caveat here about the bus factor?)</font>\n",
    "\n",
    "Commitment to stability means proposed changes must go through extensive design and review:\n",
    " - [Numpy Enhancement Proposals (NEPs)](https://numpy.org/neps/) - analogous to PEPs, specific to NumPy\n",
    " - Steering council for high-level direction and coordination with [NumFOCUS](https://numfocus.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Case-Study: `np.random`\n",
    " - Changes proposed in [NEP 19](https://numpy.org/neps/nep-0019-rng-policy.html), subsequently \"approved\" by the community after discussion on the mailing list.\n",
    " - Overhaul of `np.random` landed in version 1.17\n",
    " \n",
    "   * Improve *performance* and *flexibility* without sacrificing stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Generate 1,000,000 random numbers the old way\n",
    "old_rands = np.random.random(int(1e6))\n",
    "print(\"Uniform random numbers from legacy np.random.random:\\n  {}\".format(old_rands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# ... and the new way\n",
    "from numpy.random import PCG64, Generator\n",
    "rg = Generator(PCG64())\n",
    "new_rands = rg.random(int(1e6))\n",
    "print(\"Uniform random numbers with new tools:\\n  {}\".format(new_rands))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Compatibility\n",
    "\n",
    "Before version 1.17, `numpy.random` relied on `RandomState` to configure and produce random numbers.\n",
    "\n",
    "There are many, many LOC (both in test suites and in production) that depend on the original `numpy.random`, so both the *interface* and the *results* must remain unchanged\n",
    " * <font color=\"green\">**Upside: Stability**</font> - output of `np.random` remains consistent with previous versions\n",
    " * <font color=\"orange\">**Downside: Discoverability**</font> - users need to know about new interface to access improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Choose a seed for generator\n",
    "seed = 1817\n",
    "\n",
    "# Random numbers generated by np.random in v1.15\n",
    "rands_from_v1_15 = np.load('data/npy_v1.15_random_seed1817_1000samples.npy')\n",
    "# Generate random numbers with legacy interface\n",
    "np.random.seed(seed)\n",
    "legacy_rands = np.random.random(1000)\n",
    "\n",
    "print(\"Arrays equivalent: \", np.allclose(rands_from_v1_15, legacy_rands))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is possible (though clunky) to replicate legacy behavior with new interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "seed = 1817\n",
    "\n",
    "from numpy.random import MT19937, RandomState\n",
    "# Set random state with legacy seeding\n",
    "rs = RandomState(seed)\n",
    "mt = MT19937()\n",
    "mt.state = rs.get_state()\n",
    "\n",
    "# New interface for generation\n",
    "rg = Generator(mt)\n",
    "mt_rands = rg.random(1000)\n",
    "print(\"Legacy: {}\\nGenerator: {}\".format(legacy_rands[:4], mt_rands[:4]))\n",
    "print(\"Arrays equivalent: \", np.allclose(legacy_rands, mt_rands))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Performance\n",
    "\n",
    "The [PCG64](https://docs.scipy.org/doc/numpy/reference/random/bit_generators/pcg64.html) BitGenerator is a \n",
    "[significant improvment](http://www.pcg-random.org/) over the legacy Marsenne Twister in many areas, including speed:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#NOTE: PCG64 is the new default bit_generator, so default_rng() equivalent to Generator(PCG64())\n",
    "from numpy.random import default_rng\n",
    "rg = default_rng()\n",
    "num_samples = int(1e5)\n",
    "\n",
    "print(\"Uniform random numbers:\")\n",
    "%timeit np.random.random(num_samples)\n",
    "%timeit rg.random(num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In addition, `Generator` includes improved methods for drawing samples from distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Standard Normal:\")\n",
    "%timeit np.random.standard_normal(num_samples)\n",
    "%timeit rg.standard_normal(num_samples) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Standard Exponential:\")\n",
    "%timeit np.random.standard_exponential(num_samples)\n",
    "%timeit rg.standard_exponential(num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Standard Gamma:\")\n",
    "shape_param = 3.0\n",
    "%timeit np.random.standard_gamma(shape_param, num_samples)\n",
    "%timeit rg.standard_gamma(shape_param, num_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What's next for NumPy?\n",
    "\n",
    "![NumpyRoadmapGraphic](./images/numpy_roadmap_graphic.png)\n",
    "\n",
    "Image from [this PyData Amsterdam 2019 presentation](https://www.slideshare.net/RalfGommers/the-evolution-of-array-computing-in-python/14) by [Ralf Gommers](https://github.com/rgommers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interoperability\n",
    "\n",
    "Separate NumPy API from NumPy \"execution engine\"\n",
    " - Allow other libraries ([Dask](https://dask.org/), [CuPy](https://cupy.chainer.org/), [PyTorch](https://pytorch.org/)) to reuse NumPy API\n",
    " - Mitigate ecosystem fragmentation\n",
    "   * E.g. don't want a re-implementation of `scipy` for each ML framework (`pytorch.scipy`, `tensorflow.scipy`, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Current n-dimensional array landscape\n",
    "\n",
    "![arrays_now](images/array_landscape_now.png)\n",
    "\n",
    "Images from this [talk at PyData NY 2019](https://www.slideshare.net/RalfGommers/pydata-nyc-whatsnew-numpyscipy-2019?next_slideshow=1) by [Ralf Gommers](https://github.com/rgommers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Vision for the future\n",
    "\n",
    "![array_vision](images/array_landscape_vision.png)\n",
    "\n",
    "Images from this [talk at PyData NY 2019](https://www.slideshare.net/RalfGommers/pydata-nyc-whatsnew-numpyscipy-2019?next_slideshow=1) by [Ralf Gommers](https://github.com/rgommers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## First solution: `__array_function__` protocol\n",
    "\n",
    " - Proposed in [NEP 18](https://numpy.org/neps/nep-0018-array-function-protocol.html)\n",
    " - Array function protocol enabled by default as of version 1.17\n",
    " \n",
    "![array_function_protocol](images/array_function_descr.png)\n",
    " \n",
    "Image source: [this presentation](https://www.slideshare.net/RalfGommers/arrayfunction-conceptual-design-related-concepts?from_action=save) by [Ralf Gommers](https://github.com/rgommers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `__array_function__` example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rg = np.random.default_rng()\n",
    "x = rg.random((5000, 1000))\n",
    "\n",
    "# Factorize with np.linalg\n",
    "q, r = np.linalg.qr(x)\n",
    "type(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "\n",
    "d = da.from_array(x, chunks=(1000, 1000))\n",
    "\n",
    "# Same call signature!\n",
    "q, r = np.linalg.qr(d)\n",
    "type(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "da.core.Array??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lessons learned from `__array_function__`\n",
    "\n",
    " - The `__array_function__` protocol has been partially successful, but has fallen short of universal adoption.\n",
    " - Valuable feedback from the community has resulted in [NEP 37](https://numpy.org/neps/nep-0037-array-module.html)\n",
    "   * Defines `__array_module__` protocol\n",
    "   * Currently under development (interested?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overhaul of the data type system\n",
    "\n",
    "The NumPy C-API allows for user-defined dtypes, but some components (e.g. some logic for casting rules) are not currently extensible by user-defined types. A [NEP is currently being drafted](https://github.com/numpy/numpy/blob/a111b551ae940d7d5f8523fef1cf3589c6ba00a0/doc/neps/nep-0033-extensible-dtypes.rst) to overhaul the dtype system. The proposal aims to:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Improve NumPy maintainability\n",
    " * Improve organization of dtype checking/comparison machinery\n",
    " * Use the same API for built-in and user-defined dtypes\n",
    " * Improve extensibility of API: facilitate future additions/modifications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### User impact\n",
    " - Easier-to-use mechanism for defining fully-feature dtypes (including from Python)\n",
    " - Host of new dtypes for the ecosystem:\n",
    "   * Physical units (cf. [astropy.unit](https://docs.astropy.org/en/stable/units/))\n",
    "   * `bfloat16`, `int24`, etc.\n",
    "   * Categorical types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Improved SIMD incorporation for `ufuncs`\n",
    "\n",
    "Strike a balance between **optimization** and **maintainability**\n",
    "\n",
    " - Define set of architecture-agnostic universal intrinsics\n",
    "   * At build time, build code paths based on features available for the host architecture\n",
    "   * At run time, detect which features are available and select which of available code paths to use\n",
    " - In the process of being formalized in a [draft NEP](https://github.com/mattip/numpy/blob/nep_simd/doc/neps/nep-XXXX-SIMD-optimizations.rst)\n",
    "   * Preliminary work in support of this proposed enhancment can be found [here](https://github.com/numpy/numpy/pull/13421/files) and [here](https://github.com/numpy/numpy/pull/13516)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Slides on indexing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supporting language features: type annotations\n",
    "\n",
    "Thinking about how best to support type annotations became especially important when they became an official core language feature in Python 3.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Work on type annotations is located in the [numpy-stubs](https://github.com/numpy/numpy-stubs) repository. Basic type annotations are supported. Here's the contents of `type_annotations.py`:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def my_strict_sin(x: np.ndarray) -> np.ndarray:\n",
    "    return np.sin(x)\n",
    "\n",
    "def my_chill_sin(x: np.array_like) -> np.array_like:\n",
    "    return np.sin(x)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    a = np.arange(10)\n",
    "    l = [1, 2, 3]\n",
    "    my_strict_sin(a)     # Passes typecheck\n",
    "    my_strict_sin(l)     # Fails typecheck\n",
    "    my_chill_sin(a)      # Passes typecheck\n",
    "    my_chill_sin(l)      # Passes typecheck\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!mypy type_annotations.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond the horizon... NumPy 2.0?\n",
    "\n",
    " - Major revision -> opportunity for refactoring/enhancements that break API\n",
    "   * Weigh potential for improvements against the pain of breaking changes (example: Python2 -> Python3)\n",
    " - So much new functionality being developed in external libraries\n",
    "   * Changes that facilitate external development are priorities\n",
    " \n",
    "A bit of the history surrounding the idea of NumPy 2.0 can be found [here](https://github.com/numpy/numpy/issues/9066)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Getting involved\n",
    "\n",
    "NumPy presents an opportunity to work on a project that is depended on by tens of millions of users (and counting). Here's how you can get involved:\n",
    " 1. Contribute\n",
    "   - [GitHub Issues](https://github.com/numpy/numpy/issues) and [open PRs](https://github.com/numpy/numpy/pulls) are a great entry point\n",
    "     * If you want to get your hands dirty immediately, try starting with the [good first issue](https://github.com/numpy/numpy/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22) label\n",
    "     * For challenges with a greater scope, try the [Enhancement](https://github.com/numpy/numpy/labels/01%20-%20Enhancement) or [Wish List](https://github.com/numpy/numpy/labels/23%20-%20Wish%20List) labels\n",
    "   - Check out the discussion revolving around accepted and proposed [NEPs](https://numpy.org/neps/)\n",
    " 2. Participate in the conversation\n",
    "  - [Numpy discussion mailing list](https://www.scipy.org/scipylib/mailing-lists.html)\n",
    "  - Numpy community meetings (links and cadence here)\n",
    "  - slack channel"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
